# ‚ú® Aura ‚Äì Your Personal AI Assistant

Aura is a customizable personal AI assistant built with **React** and powered by Google's **Gemini Generative AI APIs**. This project provides a solid foundation for creating your own conversational AI, complete with both text and high-quality voice responses.

-----

## üöÄ Features

  - **Conversational AI:** Interact with the `gemini-2.5-flash-preview-05-20` model for dynamic conversations.
  - **Customizable Personality:** The assistant's persona is defined in a separate file, making it easy to configure its tone and style.
  - **Text-to-Speech (TTS) Playback:** Hear responses with high-quality audio generated by `gemini-2.5-flash-preview-tts`.
  - **Modern Stack:** Built with **React** for a responsive UI and **TailwindCSS** for streamlined styling.
  - **Scalable Architecture:** The modular API structure, including externalized configurations, makes it easy to add new features or integrate with different models and voices.

-----

## üõ†Ô∏è Project Setup

### 1\. Clone the repo

```bash
git clone https://github.com/your-username/aura-assistant.git
cd aura-assistant
```

### 2\. Install dependencies

```bash
npm install
```

### 3\. Setup environment variables

Create a `.env` file in the project root to store your API key:

```env
VITE_GEMINI_API_KEY=your_google_ai_studio_api_key_here
```

üîë **Get your API key**:

1.  Go to [Google AI Studio](https://aistudio.google.com/).
2.  Create a new project and enable the **Gemini API**.
3.  Generate an API key.
4.  Copy and paste it into your `.env` file.

-----

## üß† Generative AI API Usage

This project demonstrates two core API integrations: the conversational chat API and the text-to-speech API.

### üîπ Chat API Example (`src/api/chatApi.js`)

This API handles the core conversation. The assistant's persona is now imported from a separate file, making it easy to swap out its personality without changing the main API logic.

```js
import { systemInstruction } from "./insrtuctions";

const apiKey = import.meta.env.VITE_GEMINI_API_KEY;

// ‚úÖ Model config (easy to change later)
const CHAT_MODEL = "gemini-2.5-flash-preview-05-20";
const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${CHAT_MODEL}:generateContent?key=${apiKey}`;
export async function sendChatMessage(history, userPrompt) {
  const payload = {
    contents: [...history, { role: "user", parts: [{ text: userPrompt }] }],
    systemInstruction: {
      parts: [
        {
          text: systemInstruction,
        },
      ],
    },
  };

  const response = await fetch(apiUrl, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(payload),
  });

  if (!response.ok) {
    throw new Error(`Chat API Error: ${response.statusText}`);
  }

  const result = await response.json();
  return result.candidates?.[0]?.content?.parts?.[0]?.text || null;
}
```

-----

### üîπ TTS API Example (`src/api/ttsApi.js`)

This API converts the text response from the chat API into playable audio. The `playAudioFromText` function handles the entire process, and the `voiceName` constant makes it simple to switch between different voices like **"Kore"**, **"Leda"**, or **"Zephyr"**.

```js
const apiKey = import.meta.env.VITE_GEMINI_API_KEY;

// ‚úÖ Model config for TTS
const TTS_MODEL = "gemini-2.5-flash-preview-tts";
const ttsApiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${TTS_MODEL}:generateContent?key=${apiKey}`;
const voiceName="Kore" // e.g. Kore, Leda, Zephyr, etc
function base64ToArrayBuffer(base64) {
  const binary = atob(base64);
  const bytes = new Uint8Array(binary.length);
  for (let i = 0; i < binary.length; i++) bytes[i] = binary.charCodeAt(i);
  return bytes.buffer;
}

function pcmToWav(pcmData, sampleRate) {
  const wavData = new ArrayBuffer(44 + pcmData.length * 2);
  const view = new DataView(wavData);

  const writeString = (offset, str) => {
    for (let i = 0; i < str.length; i++) view.setUint8(offset + i, str.charCodeAt(i));
  };

  writeString(0, "RIFF");
  view.setUint32(4, 36 + pcmData.length * 2, true);
  writeString(8, "WAVE");
  writeString(12, "fmt ");
  view.setUint32(16, 16, true);
  view.setUint16(20, 1, true);
  view.setUint16(22, 1, true);
  view.setUint32(24, sampleRate, true);
  view.setUint32(28, sampleRate * 2, true);
  view.setUint16(32, 2, true);
  view.setUint16(34, 16, true);
  writeString(36, "data");
  view.setUint32(40, pcmData.length * 2, true);

  let offset = 44;
  for (let i = 0; i < pcmData.length; i++, offset += 2) {
    view.setInt16(offset, pcmData[i], true);
  }

  return new Blob([view], { type: "audio/wav" });
}

export async function playAudioFromText(text) {
  const payload = {
    contents: [{ parts: [{ text }] }],
    generationConfig: {
      responseModalities: ["AUDIO"],
      speechConfig: {
        voiceConfig: { prebuiltVoiceConfig: { voiceName } } 
      }
    },
    model: "gemini-2.5-flash-preview-tts"
  };

  const response = await fetch(ttsApiUrl, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(payload)
  });

  if (!response.ok) {
    throw new Error(`TTS API Error: ${response.statusText}`);
  }

  const result = await response.json();
  const part = result?.candidates?.[0]?.content?.parts?.[0];
  const audioData = part?.inlineData?.data;
  const mimeType = part?.inlineData?.mimeType;

  if (audioData && mimeType?.startsWith("audio/")) {
    const sampleRateMatch = mimeType.match(/rate=(\d+)/);
    const sampleRate = sampleRateMatch ? parseInt(sampleRateMatch[1], 10) : 16000;

    const pcmData = base64ToArrayBuffer(audioData);
    const pcm16 = new Int16Array(pcmData);
    const wavBlob = pcmToWav(pcm16, sampleRate);
    const audioUrl = URL.createObjectURL(wavBlob);

    const audio = new Audio(audioUrl);
    await audio.play();
  } else {
    console.error("No audio returned from TTS API.");
  }
}
```

-----

## ‚ñ∂Ô∏è Running the Project

```bash
npm run dev
```

Then open **[http://localhost:5173](https://www.google.com/search?q=http://localhost:5173)** in your browser.

-----

## üìù Example Chat

**You:** "What is prompt engineering?"
**Aura:**

```
Prompt engineering is like teaching a computer how to think. You give it very specific instructions, called prompts, to get the exact answer you want from an AI. It's the art of clear communication with a machine. üß†‚ú®
```

(‚ú® Also speaks it out loud using TTS)

-----

## üîÆ Future Enhancements

  * **Multi-turn Conversations:** Implement persisted context to support longer, more complex conversations.
  * **Voice Input:** Add a Speech-to-Text (STT) feature to create a complete voice-in, voice-out loop.
  * **Export Chat History:** Allow users to save or export their chat history.
  * **More Voice Options:** Integrate options for different voices or languages from the TTS API.